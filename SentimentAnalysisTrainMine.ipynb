{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/MobileHome/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/MobileHome/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/MobileHome/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#downloaded in MobileHome nltk_data: corpora, taggers (perceptron), tokenizers(punkt)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer,  TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://pythonprogramming.net/new-data-set-training-nltk-tutorial/\n",
    "# training is done on a movie reviews dataset, provided in the website above\n",
    "# the movie review dataset is composed of short sentences of lenght comparable to the spooky author dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open(\"short_reviews/positive.txt\",\"r\", encoding = \"ISO-8859-1\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",\"r\", encoding = \"ISO-8859-1\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whaley's determination to immerse you in sheer...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suffers from over-familiarity since hit-hungry...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a distinctly mixed bag , the occasional bursts...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an endlessly fascinating , landmark movie that...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ramsay , as in ratcatcher , remains a filmmake...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>at least it's a fairly impressive debut from t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>a huge box-office hit in korea , shiri is a mu...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>while not all that bad of a movie , it's nowhe...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10662</th>\n",
       "      <td>this is a film living far too much in its own ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10663</th>\n",
       "      <td>it's a compelling and horrifying story , and t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10664 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text review\n",
       "0      whaley's determination to immerse you in sheer...    neg\n",
       "1      suffers from over-familiarity since hit-hungry...    neg\n",
       "2      a distinctly mixed bag , the occasional bursts...    neg\n",
       "3      an endlessly fascinating , landmark movie that...    pos\n",
       "4      ramsay , as in ratcatcher , remains a filmmake...    pos\n",
       "...                                                  ...    ...\n",
       "10659  at least it's a fairly impressive debut from t...    neg\n",
       "10660  a huge box-office hit in korea , shiri is a mu...    pos\n",
       "10661  while not all that bad of a movie , it's nowhe...    pos\n",
       "10662  this is a film living far too much in its own ...    neg\n",
       "10663  it's a compelling and horrifying story , and t...    pos\n",
       "\n",
       "[10664 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Building Procedure 2\n",
    "data=pd.DataFrame(documents)\n",
    "data.columns= ['text', 'review']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def clean(doc):\n",
    "    punc_free = ''.join(ch for ch in doc if ch not in exclude)\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    # POS_LIST = [NOUN, VERB, ADJ, ADV]\n",
    "    # lemma.lemmatize(\"rocks\")\n",
    "    # lemma.lemmatize(\"better\", pos=\"a\")\n",
    "    normalized_1 = \" \".join(lemma.lemmatize(word, 'a') for word in punc_free.split()) \n",
    "    normalized_2 = \" \".join(lemma.lemmatize(word, 'n') for word in normalized_1.split()) #this deleted s from as\n",
    "    normalized_3 = \" \".join(stemmer.stem(word) for word in normalized_2.split()) #this is very effective\n",
    "    stop_free = \" \".join([i for i in normalized_3.lower().split() if i not in stop])\n",
    "    return stop_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].apply(clean) #clean(data['text'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    \"\"\"\n",
    "    Count vectorizing function.  Returns embedded vector and vectorizer.\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer() # Bag-of-words vectorization\n",
    "#     count_vectorizer = TfidfVectorizer() # Term Frequency-Inverse Document Frequency vectorization\n",
    "    #count_vectorizer = CountVectorizer(ngram_range =(1, 2)) # n-gram vectorization  \n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10664, 13927)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb, dictionary = cv(data['text_clean'].tolist())\n",
    "emb.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MobileHome/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/MobileHome/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/MobileHome/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/MobileHome/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/MobileHome/anaconda3/envs/tf2/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def gen_Classifier(data, emb, classifier):     \n",
    "    # freq= pd.Series((' '.join(data['text']).split())).valuecounts().sortvalues(ascending = False)\n",
    "    # vocab_ = barplotTopWords(freq)\n",
    "    array = emb.todense()\n",
    "    emb2= pd.DataFrame(array)\n",
    "    emb2['output'] = data['review']\n",
    "    \n",
    "\n",
    "    features = emb2.columns.tolist()\n",
    "    output = 'output'\n",
    "    # removing the output and the id from features\n",
    "    features.remove(output)\n",
    "    \n",
    "    if classifier == RandomForestClassifier:\n",
    "        list1 = [False, True]\n",
    "        parameter_grid =[{\"bootstrap\": list1 }]\n",
    "        score= 'accuracy'\n",
    "    elif classifier == MultinomialNB:\n",
    "        list1 = np.linspace(0.006, 0.1, 2)\n",
    "        list1 = np.around(list1, decimals=4)\n",
    "        parameter_grid =[{\"alpha\": list1}]\n",
    "        score= 'accuracy'\n",
    "        \n",
    "    classifier1= classifier()\n",
    "    gridsearch1 = GridSearchCV(classifier1,parameter_grid, scoring = score, cv = 2)\n",
    "    \n",
    "    my_model= gridsearch1.fit(emb2[features], emb2[output])\n",
    "    return my_model\n",
    "\n",
    "gridsearch1 = gen_Classifier(data, emb, RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch1.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13918</th>\n",
       "      <th>13919</th>\n",
       "      <th>13920</th>\n",
       "      <th>13921</th>\n",
       "      <th>13922</th>\n",
       "      <th>13923</th>\n",
       "      <th>13924</th>\n",
       "      <th>13925</th>\n",
       "      <th>13926</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13928 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  13918  13919  13920  13921  13922  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "\n",
       "   13923  13924  13925  13926  output  \n",
       "0      0      0      0      0     neg  \n",
       "1      0      0      0      0     pos  \n",
       "2      0      0      0      0     neg  \n",
       "3      0      0      0      0     pos  \n",
       "4      0      0      0      0     neg  \n",
       "\n",
       "[5 rows x 13928 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_set = pd.DataFrame (emb.todense() [:round(emb.shape[0]/2)])\n",
    "# testing_set = pd.DataFrame (emb.todense() [round(emb.shape[0]/2):])\n",
    "\n",
    "array = emb.todense()\n",
    "emb2= pd.DataFrame(array)\n",
    "emb2['output'] = data['review']\n",
    "    \n",
    "\n",
    "features = emb2.columns.tolist()\n",
    "output = 'output'\n",
    "# removing the output and the id from features\n",
    "features.remove(output)\n",
    "\n",
    "emb2.head()\n",
    "\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = emb2 [:round(emb.shape[0]/2)]\n",
    "testing_set = emb2 [round(emb.shape[0]/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Classifier(testing_set, model):\n",
    "    prediction= model.predict(testing_set)\n",
    "#     if model.estimator == RandomForestClassifier:\n",
    "#         results2= model.best_estimator_.feature_importances_\n",
    "#     results2=(nltk.classify.accuracy(model, testing_set))*100\n",
    "#     elif model.estimator == MultinomialNB:\n",
    "#     results2= model.best_params_\n",
    "        \n",
    "    return prediction, results2, model.best_params_, model.best_score_ , model.best_estimator_  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] {'bootstrap': False} 0.6927044261065266 RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "prediction, results2, best_params, best_score, best_estimator= predict_Classifier(testing_set[features], gridsearch1)\n",
    "print(results2, best_params, best_score, best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998124531132783"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct= sum(prediction == data['review'][round(emb.shape[0]/2):])\n",
    "me= n_correct/len(prediction)\n",
    "me\n",
    "#this is so high because I am testing on the same dataset as the training set, \n",
    "#should I only consider the value from the GRIDSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresimportance= gridsearch1.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13927"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictnames= dictionary.get_feature_names()\n",
    "len(dictnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureimportancewithnames = pd.DataFrame(np.column_stack([dictnames, featuresimportance]), \n",
    "                               columns=['name', 'importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>compress</td>\n",
       "      <td>9.999802672255283e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>industri</td>\n",
       "      <td>9.986964826582374e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>sloppi</td>\n",
       "      <td>9.985509633278748e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>suicid</td>\n",
       "      <td>9.976125352183732e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>coal</td>\n",
       "      <td>9.974601983605891e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name             importance\n",
       "2559   compress  9.999802672255283e-07\n",
       "6173   industri  9.986964826582374e-07\n",
       "11191    sloppi  9.985509633278748e-05\n",
       "11876    suicid  9.976125352183732e-06\n",
       "2400       coal  9.974601983605891e-07"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mostimp= featureimportancewithnames.sort_values( ['importance'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "Mostimp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_classifier = open(\"pickled/SentimentAnalysisTrainClassifierMine.pickle\",\"wb\")\n",
    "pickle.dump(gridsearch1, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier_f = open(\"pickled/SentimentAnalysisTrainClassifierMine.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13917</th>\n",
       "      <th>13918</th>\n",
       "      <th>13919</th>\n",
       "      <th>13920</th>\n",
       "      <th>13921</th>\n",
       "      <th>13922</th>\n",
       "      <th>13923</th>\n",
       "      <th>13924</th>\n",
       "      <th>13925</th>\n",
       "      <th>13926</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5332 rows Ã— 13927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9      \\\n",
       "0         0      0      0      0      0      0      0      0      0      0   \n",
       "1         0      0      0      0      0      0      0      0      0      0   \n",
       "2         0      0      0      0      0      0      0      0      0      0   \n",
       "3         0      0      0      0      0      0      0      0      0      0   \n",
       "4         0      0      0      0      0      0      0      0      0      0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5327      0      0      0      0      0      0      0      0      0      0   \n",
       "5328      0      0      0      0      0      0      0      0      0      0   \n",
       "5329      0      0      0      0      0      0      0      0      0      0   \n",
       "5330      0      0      0      0      0      0      0      0      0      0   \n",
       "5331      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      ...  13917  13918  13919  13920  13921  13922  13923  13924  13925  \\\n",
       "0     ...      0      0      0      0      0      0      0      0      0   \n",
       "1     ...      0      0      0      0      0      0      0      0      0   \n",
       "2     ...      0      0      0      0      0      0      0      0      0   \n",
       "3     ...      0      0      0      0      0      0      0      0      0   \n",
       "4     ...      0      0      0      0      0      0      0      0      0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5327  ...      0      0      0      0      0      0      0      0      0   \n",
       "5328  ...      0      0      0      0      0      0      0      0      0   \n",
       "5329  ...      0      0      0      0      0      0      0      0      0   \n",
       "5330  ...      0      0      0      0      0      0      0      0      0   \n",
       "5331  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      13926  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "5327      0  \n",
       "5328      0  \n",
       "5329      0  \n",
       "5330      0  \n",
       "5331      0  \n",
       "\n",
       "[5332 rows x 13927 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7554388597149287\n",
      "0.8998499624906227\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = MultinomialNB()\n",
    "MNB_classifier.fit(training_set[features], training_set[output])\n",
    "predicted = MNB_classifier.predict(testing_set[features])\n",
    "print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "BernoulliNB_classifier = BernoulliNB()\n",
    "BernoulliNB_classifier.fit(training_set[features], training_set[output])\n",
    "predicted = classifier1.predict(testing_set[features])\n",
    "print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "# LogisticRegression_classifier = LogisticRegression()\n",
    "# LogisticRegression_classifier.fit(training_set[features], training_set[output])\n",
    "# predicted = classifier1.predict(testing_set[features])\n",
    "# print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "# SGDClassifier_classifier = SGDClassifier()\n",
    "# SGDClassifier_classifier.fit(training_set[features], training_set[output])\n",
    "# predicted = classifier1.predict(testing_set[features])\n",
    "# print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "##SVC_classifier = SVC()\n",
    "##SVC_classifier.fit(training_set[features], training_set[output])\n",
    "# predicted = classifier1.predict(testing_set[features])\n",
    "# print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "# LinearSVC_classifier = LinearSVC()\n",
    "# LinearSVC_classifier.fit(training_set[features], training_set[output])\n",
    "# predicted = classifier1.predict(testing_set[features])\n",
    "# print (accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "# NuSVC_classifier = NuSVC()\n",
    "# NuSVC_classifier.fit(training_set[features], training_set[output])\n",
    "# predicted = classifier1.predict(testing_set[features])\n",
    "# print (accuracy_score(testing_set[output], predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: VoteClassifier 0.8998499624906227\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "#                                   NuSVC_classifier,\n",
    "#                                   LinearSVC_classifier,\n",
    "#                                   SGDClassifier_classifier,\n",
    "#                                   LogisticRegression_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  MNB_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", voted_classifier.__class__.__name__, accuracy_score(testing_set[output], predicted))\n",
    "\n",
    "voted_classifier.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open(\"pickled/SentimentAnalysisTrainMNB_classifier.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "save_classifier = open(\"pickled/SentimentAnalysisTrainBernoulliNB_classifier.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NOT USED\n",
    "# all_words = []\n",
    "\n",
    "# short_pos_words = nltk.word_tokenize(short_pos)\n",
    "# short_neg_words = nltk.word_tokenize(short_neg)\n",
    "\n",
    "# for w in short_pos_words:\n",
    "#     all_words.append(w.lower())\n",
    "\n",
    "# for w in short_neg_words:\n",
    "#     all_words.append(w.lower())\n",
    "\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "# all_words\n",
    "# len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move this up here\n",
    "# all_words = []\n",
    "# documents = []\n",
    "\n",
    "\n",
    "# #  j is adject, r is adverb, and v is verb\n",
    "# #allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "# allowed_word_types = [\"J\"]\n",
    "\n",
    "# for p in short_pos.split('\\n'):\n",
    "#     documents.append( (p, \"pos\") )\n",
    "#     words = word_tokenize(p)\n",
    "#     pos = nltk.pos_tag(words)\n",
    "#     for w in pos:\n",
    "#         if w[1][0] in allowed_word_types:\n",
    "#             all_words.append(w[0].lower())\n",
    "\n",
    "\n",
    "# for p in short_neg.split('\\n'):\n",
    "#     documents.append( (p, \"neg\") )\n",
    "#     words = word_tokenize(p)\n",
    "#     pos = nltk.pos_tag(words)\n",
    "#     for w in pos:\n",
    "#         if w[1][0] in allowed_word_types:\n",
    "#             all_words.append(w[0].lower())\n",
    "\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "# def find_features(document):\n",
    "#     words = word_tokenize(document)\n",
    "#     features = {}\n",
    "#     for w in word_features:\n",
    "#         features[w] = (w in words)\n",
    "\n",
    "#     return features\n",
    "\n",
    "# featuresets = [(find_features(rev), category) for (rev, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "a= MultinomialNB()\n",
    "b= BernoulliNB()\n",
    "voting= VotingClassifier(estimators=[('M', a), ('b', b)], voting= 'hard')\n",
    "\n",
    "voting.fit(testing_set[features], testing_set[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (a, b, voting):\n",
    "    clf.fit(testing_set[features], testing_set[output])\n",
    "    y_pred= clf.predict(testing_set[features])\n",
    "    print(clf.__class__.__name__, accuracy_score(testing_set[output], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
